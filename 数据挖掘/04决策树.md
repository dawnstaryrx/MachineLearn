# 决策树

## 1 基本流程

### 1.1 基本概念

1. 决策树：从训练数据中学习得出一个树状结构的模型。
2. 决策树属于“判别模型”。 
3. 决策树算法属于**监督学习**方法。
4. 决策树是一种**树状结构**，通过做出一系列的决策来对数据进行划分，这类似于针对一系列问题进行选择。
5. 决策树的**决策过程**：从根结点开始，测试待分类项中对应的特征属性，并按照属性值选择输出分支，直到叶子节点，将叶子节点的存放的类别作为决策结果。
6. 决策树算法是一种**归纳分类**算法，它通过对训练集的学习，挖掘出有用的规则，用于对新数据进行预测。
7. 决策树归纳的基本算法是**贪心算法**，自顶向下来构建决策树。
8. 在决策树的生成过程中，分割方法即**属性选择的度量**是关键。
9. 决策过程中提出的每个判定问题都是对某个属性的测试；
10. 决策过程的最终结论对应了我们希望的判定结果；
11. 从根结点到每个叶结点的路径对应了一个判定测试序列；
12. 决策树学习的**目的**是：产生一棵泛化能力强，即处理未见示例能力强的决策树。

### 1.2 特点

1. **优点**
   1. 推理过程容易理解，计算简单，可解释性强。
   2. 比较适合处理有缺失属性的样本。
   3. 可自动忽略目标变量没有贡献的属性变量，也为判断属性变量的重要性、减少变量的数目提供参考。
2. **缺点**
   1. 容易造成**过拟合**，需要剪枝。
   2. 忽略了数据之间的相关性，
   3. 对于各类别样本数量不一致的数据，信息增益会偏向于那些更多数值的特征。

### 1.3 三种基本类型

![img.png](img/04_1img.png)

### 1.4 步骤

1. 特征选择
   1. 特征选择决定了使用哪些特征来做判断。在训练数据集中，每个样本的属性可能有很多个，不同属性的作用有大有小。因而特征选择的作用就是筛选出跟分类结果相关性较高的特征，也就是分类能力较强的特征。
   2. 在特征选择中通常使用的准则是：信息增益。
2. 决策树生成
   1. 选择好特征后，就从根节点出发，对节点计算所有特征的信息增益，选择信息增益最大的特征作为节点特征，根据该特征的不同取值建立子节点；
   2. 对每个子节点使用相同的方式生成新的子节点，直到信息增益很小或者没有特征可以选择为止。
3. 决策树剪枝——剪枝的主要目的是对抗「过拟合」，通过主动去掉部分分支来降低过拟合的风险。

## 2 划分选择

## 3 减枝处理

## 4 连续和缺失值

## 5 多变量决策树

